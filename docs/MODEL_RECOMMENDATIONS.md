# ProGen2 Model Recommendations

## Problem: ProGen2-Small Generates Low-Quality Sequences

**Issue:** ProGen2-small (151M parameters) generates repetitive, pathological sequences:
- Long runs of single amino acids (e.g., 168 consecutive A's)
- Very low identity to baseline (~17% vs required 55-75%)
- Repetitive patterns (e.g., GSRPGSGPGSGP...)
- Only 11 unique amino acids (out of 20)

**Root Cause:** ProGen2-small is too small for generating high-quality 263aa protein sequences. The model collapses into repetitive patterns.

---

## Quick Fixes Applied

1. **Composition gate runs FIRST** (before identity) to filter repetitive junk early
2. **More conservative sampling:**
   - Conservative lane: temp 0.4 (was 0.6)
   - Exploratory lane: temp 0.8 (was 0.9)

These help, but **ProGen2-small is fundamentally too weak** for this task.

---

## Recommended: Use a Larger Model

### Option 1: ProGen2-Medium or ProGen2-Base (764M) ‚≠ê **RECOMMENDED**

**Best balance of quality and speed.**

```bash
# Download checkpoint
cd external/progen2
model=progen2-medium  # or progen2-base
mkdir -p checkpoints/${model}
curl -L -o checkpoints/${model}/${model}.tar.gz \
  https://storage.googleapis.com/sfr-progen-research/checkpoints/${model}.tar.gz
tar -xvf checkpoints/${model}/${model}.tar.gz -C checkpoints/${model}/

# Run pipeline
python scripts/run_progen2_pipeline.py run_20251230_progen2_medium_r1_test \
  --num-samples 100 \
  --model progen2-medium
```

**Expected improvements:**
- Higher identity to baseline (should hit 55-75% range)
- Less repetitive patterns
- More diverse amino acid usage
- Better sequence quality overall

**Time estimate:** ~2-3x slower than small, but still reasonable on CPU (~1-2 hours for 50 samples)

---

### Option 2: ProGen2-Large (2.7B) üöÄ **BEST QUALITY**

**Highest quality, but slower.**

```bash
# Download checkpoint (~10GB)
cd external/progen2
model=progen2-large
mkdir -p checkpoints/${model}
curl -L -o checkpoints/${model}/${model}.tar.gz \
  https://storage.googleapis.com/sfr-progen-research/checkpoints/${model}.tar.gz
tar -xvf checkpoints/${model}/${model}.tar.gz -C checkpoints/${model}/

# Run pipeline
python scripts/run_progen2_pipeline.py run_20251230_progen2_large_r1_test \
  --num-samples 50 \
  --model progen2-large
```

**Expected improvements:**
- Highest quality sequences
- Best identity to baseline
- Most natural sequences
- Best for final production runs

**Time estimate:** ~5-10x slower than small
- 50 samples: ~5-20 hours (good for overnight run)
- 100 samples: ~10-40 hours (might not finish overnight)

---

## Model Comparison

| Model | Size | Quality | Speed | Use Case |
|-------|------|---------|-------|----------|
| **progen2-small** | 151M | ‚ùå Too low | ‚ö° Fast | ‚ùå Not recommended |
| **progen2-medium** | 764M | ‚úÖ Good | ‚ö°‚ö° Medium | ‚úÖ **Recommended** |
| **progen2-base** | 764M | ‚úÖ Good | ‚ö°‚ö° Medium | ‚úÖ Alternative to medium |
| **progen2-large** | 2.7B | ‚úÖ‚úÖ Excellent | ‚ö°‚ö°‚ö° Slow | ‚úÖ Best for final runs |

---

## Strategy

1. **Start with ProGen2-medium** for pipeline validation
2. **Use ProGen2-large** for final production runs once pipeline is validated
3. **Skip ProGen2-small** - it's too weak for this task

---

## Alternative: Try Only 80aa Prompts

If you must use ProGen2-small, try:
- **Only 80aa prompts** (more conditioning = better quality)
- **Conservative lane only** (temp 0.4)
- **More samples** (200-500) to find rare good sequences

But this is **not recommended** - use a larger model instead.

---

## Overnight Run Recommendations

For ProGen2-large overnight runs:

**Recommended:** `--num-samples 50`
- Total sequences: ~300 (3 prompts √ó 2 lanes √ó 50)
- Time estimate: ~5-20 hours
- Should finish overnight (8-12 hours typical)

**If you want more:** `--num-samples 75`
- Total sequences: ~450
- Time estimate: ~7.5-30 hours
- Risky - might not finish overnight

**Note:** Each "sample" is one sequence generated by the model. The term "sample" comes from machine learning - each generated sequence is a sample from the probability distribution that ProGen2 learned during training.
